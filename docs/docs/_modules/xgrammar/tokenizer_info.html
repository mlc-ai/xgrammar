





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xgrammar.tokenizer_info &mdash; XGrammar 0.1.22 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/fix_text_selection.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/tabs.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
        <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
        <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
        <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
        <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://xgrammar.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://xgrammar.mlc.ai/docs/>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/xgrammar>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://blog.mlc.ai/>Blog</a>
                </li>
             </ul>
          </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/img/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.22
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../start/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../start/quick_start.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/constrained_decoding.html">Constrained Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/workflow_of_xgrammar.html">Workflow of XGrammar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/advanced_topics.html">Advanced Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/engine_integration.html">Integration with LLM Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/json_generation.html">JSON Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/ebnf_guided_generation.html">EBNF-Guided Generation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">XGrammar Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../xgrammar_features/runtime_safeguards.html">Runtime Safeguards</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../xgrammar_features/javascript_api.html">JavaScript API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/building_docs.html">Building Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/code_coverage.html">Code Coverage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/index.html">XGrammar Python API</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- XGrammar -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="../index.html">Module code</a> <span class="br-arrow">></span></li>
        
      <li>xgrammar.tokenizer_info</li>
    
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for xgrammar.tokenizer_info</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;This module provides the tokenizer info class to handle the tokenizer information.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">sentencepiece</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">sentencepiece</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">tiktoken</span> <span class="o">=</span> <span class="kc">None</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">XGRObject</span><span class="p">,</span> <span class="n">_core</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.support</span><span class="w"> </span><span class="kn">import</span> <span class="n">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">enable_logging</span><span class="p">()</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="VocabType"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.VocabType">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">VocabType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The type of the vocabulary. Used in TokenizerInfo. XGrammar supports three types of</span>
<span class="sd">    vocabularies: RAW, BYTE_FALLBACK, BYTE_LEVEL.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">RAW</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The vocabulary is in the raw format.</span>

<span class="sd">    The tokens in the vocabulary are kept in their original form without any processing. This kind</span>
<span class="sd">    of tokenizer includes the tiktoken tokenizer, e.g. microsoft/Phi-3-small-8k-instruct,</span>
<span class="sd">    Qwen/Qwen-7B-Chat, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BYTE_FALLBACK</span> <span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The vocabulary used in the byte fallback BPE tokenizer.</span>

<span class="sd">    The tokens are encoded through the byte-fallback conversion. E.g. &quot;\u001b&quot; -&gt; &quot;&lt;0x1B&gt;&quot;,</span>
<span class="sd">    &quot; apple&quot; -&gt; &quot;▁apple&quot;. This kind of tokenizer includes meta-llama/Llama-2-7b-chat,</span>
<span class="sd">    microsoft/Phi-3.5-mini-instruct, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BYTE_LEVEL</span> <span class="o">=</span> <span class="mi">2</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The vocabulary used in the byte level BPE tokenizer.</span>

<span class="sd">    The tokens are encoded through the byte-to-unicode conversion, as in</span>
<span class="sd">    https://github.com/huggingface/transformers/blob/87be06ca77166e6a6215eee5a990ab9f07238a18/src/transformers/models/gpt2/tokenization_gpt2.py#L38-L59</span>

<span class="sd">    This kind of tokenizer includes meta-llama/Meta-Llama-3-8B-Instruct,</span>
<span class="sd">    meta-llama/Meta-Llama-3.1-8B-Instruct, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="TokenizerInfo"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TokenizerInfo</span><span class="p">(</span><span class="n">XGRObject</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The tokenizer info contains the vocabulary, the type of the vocabulary, and necessary</span>
<span class="sd">    information for the grammar-guided generation.</span>

<span class="sd">    Note that although some tokenizers will encode the tokens in a special format, e.g.</span>
<span class="sd">    &quot;&lt;0x1B&gt;&quot; for &quot;\u001b&quot; in the ByteFallback tokenizer, and &quot;Ġ&quot; for &quot; &quot; in the Byte-Level BPE</span>
<span class="sd">    tokenizer, TokenizerInfo always decodes the vocabulary to the original format (e.g. &quot;\u001b&quot;</span>
<span class="sd">    and &quot; &quot;).</span>

<span class="sd">    Also note that some models (e.g. Phi-3 and Deepseek-V2) may pad the vocabulary to a multiple</span>
<span class="sd">    of 32. In this case, the model&#39;s vocab_size is larger than the tokenizer&#39;s vocabulary size.</span>
<span class="sd">    Please pass the model&#39;s vocab_size to the vocab_size parameter in the constructor, because</span>
<span class="sd">    this information is used to determine the size of the token mask.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TokenizerInfo.__init__"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo.__init__">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoded_vocab</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">bytes</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">vocab_type</span><span class="p">:</span> <span class="n">VocabType</span> <span class="o">=</span> <span class="n">VocabType</span><span class="o">.</span><span class="n">RAW</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_prefix_space</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct the tokenizer info.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        encoded_vocab : Union[List[bytes], List[str]]</span>
<span class="sd">            The encoded vocabulary of the tokenizer.</span>

<span class="sd">        vocab_type : VocabType, default: VocabType.RAW</span>
<span class="sd">            The type of the vocabulary. See also VocabType.</span>

<span class="sd">        vocab_size : Optional[int], default: None</span>
<span class="sd">            The size of the vocabulary. If not provided, the vocabulary size will be len(encoded_vocab).</span>

<span class="sd">        stop_token_ids : Optional[List[int]], default: None</span>
<span class="sd">            The stop token ids. If not provided, the stop token ids will be auto detected (but may not</span>
<span class="sd">            be correct).</span>

<span class="sd">        add_prefix_space : bool, default: False</span>
<span class="sd">            Whether the tokenizer will prepend a space before the text in the tokenization process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stop_token_ids</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">stop_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">stop_token_ids</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_handle</span><span class="p">(</span>
            <span class="n">_core</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="p">(</span>
                <span class="n">encoded_vocab</span><span class="p">,</span> <span class="n">vocab_type</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">stop_token_ids</span><span class="p">,</span> <span class="n">add_prefix_space</span>
            <span class="p">)</span>
        <span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_is_tiktoken_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tiktoken</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># helper to check if tokenizer is a tiktoken tokenizer</span>
        <span class="n">has_tiktoken_encoding</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">Encoding</span>
        <span class="p">)</span>

        <span class="n">filename_pattern</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;vocab_files_names&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="s2">&quot;vocab_file&quot;</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_files_names</span>
            <span class="ow">and</span> <span class="s2">&quot;tiktoken&quot;</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">has_tiktoken_encoding</span> <span class="ow">or</span> <span class="n">filename_pattern</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_is_sentencepiece_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sentencepiece</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># helper to check if tokenizer is a sentence piece tokenizer</span>
        <span class="n">has_sp_model_attr</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;sp_model&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">sp_model</span><span class="p">,</span> <span class="n">sentencepiece</span><span class="o">.</span><span class="n">SentencePieceProcessor</span>
        <span class="p">)</span>

        <span class="n">has_nested_sp_model_attr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;sp_model&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">sp_model</span><span class="p">,</span> <span class="n">sentencepiece</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">has_sp_model_attr</span> <span class="ow">or</span> <span class="n">has_nested_sp_model_attr</span>

<div class="viewcode-block" id="TokenizerInfo.from_huggingface"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo.from_huggingface">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_huggingface</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TokenizerInfo&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct the tokenizer info from the huggingface tokenizer. This constructor supports</span>
<span class="sd">        various tokenizer backends, including the huggingface fast tokenizer and tiktoken tokenizer.</span>
<span class="sd">        Necessary information is automatically detected from the tokenizer.</span>

<span class="sd">        The vocab_size parameter is introduced to handle the misalignment between the model&#39;s</span>
<span class="sd">        vocab_size and the tokenizer&#39;s vocabulary size. User should pass the model&#39;s vocab_size</span>
<span class="sd">        (could be defined in the model config) here. See docs of vocab_size for more details.</span>

<span class="sd">        The stop token ids is by default the eos_token_id of the tokenizer. If there are other</span>
<span class="sd">        stop tokens, you can specify them manually.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tokenizer : PreTrainedTokenizerBase</span>
<span class="sd">            The huggingface tokenizer.</span>

<span class="sd">        vocab_size : Optional[int], default: None</span>
<span class="sd">            The vocabulary size **defined by the model** (**not the tokenizer**). This equals to the</span>
<span class="sd">            vocab dimention of the model&#39;s lm_head. This is the size of the token mask.</span>

<span class="sd">            It can be:</span>

<span class="sd">            1. the same as the tokenizer&#39;s vocabulary size. This is the most common case.</span>
<span class="sd">            2. larger than the tokenizer&#39;s vocabulary size. This happens when the model has padding</span>
<span class="sd">               to lm_head, possibly due to aligning lm_head to the power of 2.</span>
<span class="sd">               E.g. Phi-3 and Deepseek-V2.</span>
<span class="sd">            3. smaller than the tokenizer&#39;s vocabulary size. This happens when the tokenizer has</span>
<span class="sd">               some added tokens that will not supported by the model. E.g.</span>
<span class="sd">               Llama-3.2 Vision and Molmo-72B-0924 has padded `&lt;|image|&gt;` tokens, but they will not</span>
<span class="sd">               be considered in lm_head or generated by the model.</span>

<span class="sd">            model_vocab_size need to be provided for case 2 and 3. If not provided, it will be</span>
<span class="sd">            set to the tokenizer&#39;s vocabulary size.</span>

<span class="sd">        stop_token_ids : Optional[List[int]], default: None</span>
<span class="sd">            The stop token ids. If not provided, the eos_token_id of the tokenizer will be used.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tokenizer_info : TokenizerInfo</span>
<span class="sd">            The tokenizer info.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stop_token_ids</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">stop_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">stop_token_ids</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stop_token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stop_token_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;stop_token_ids cannot be empty&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">vocab_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot get the vocabulary of the tokenizer </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span><span class="si">}</span><span class="s2">. The tokenizer &quot;</span>
                <span class="s2">&quot;should have a get_vocab method.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>

        <span class="c1"># Some tokenizer don&#39;t have token id 0 or 1 or 2. So the max_id could be larger than the</span>
        <span class="c1"># number of tokens.</span>
        <span class="n">max_id</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">vocab_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">tokenizer_vocab_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">),</span> <span class="n">max_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="ow">or</span> <span class="n">tokenizer_vocab_size</span>

        <span class="c1"># maintain tokenizer&#39;s indexing</span>
        <span class="n">encoded_vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">vocab_size</span>
        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">:</span>
                <span class="n">encoded_vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">token</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">):</span>
            <span class="c1"># huggingface fast tokenizer</span>
            <span class="c1"># - the vocabulary is directly obtained from tokenizer.get_vocab()</span>
            <span class="c1">#   (tokenizer.backend_tokenizer.to_str() may not contain the full vocab, special</span>
            <span class="c1">#   tokens may be omitted)</span>
            <span class="c1"># - the vocab size is obtained from len(tokenizer.get_vocab()) or provided by user</span>
            <span class="c1"># - the vocab type and add_prefix_space are obtained from</span>
            <span class="c1">#   tokenizer.backend_tokenizer.to_str()</span>
            <span class="c1"># - stop token id is provided by user, or auto detected.</span>
            <span class="n">backend_str</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">backend_tokenizer</span><span class="o">.</span><span class="n">to_str</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">stop_token_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stop_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;When constructing TokenizerInfo from a huggingface tokenizer, &quot;</span>
                        <span class="s2">&quot;stop_token_ids is neither provided by user nor found from the tokenizer. &quot;</span>
                        <span class="s2">&quot;It will be automatically detected.&quot;</span>
                    <span class="p">)</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">_detect_metadata_from_hf</span><span class="p">(</span><span class="n">backend_str</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">TokenizerInfo</span><span class="p">(</span>
                <span class="n">encoded_vocab</span><span class="p">,</span>
                <span class="n">vocab_type</span><span class="o">=</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;vocab_type&quot;</span><span class="p">],</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">stop_token_ids</span><span class="o">=</span><span class="n">stop_token_ids</span><span class="p">,</span>
                <span class="n">add_prefix_space</span><span class="o">=</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;add_prefix_space&quot;</span><span class="p">],</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">_is_tiktoken_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">):</span>
            <span class="c1"># tiktoken tokenizer</span>
            <span class="c1"># e.g. Phi-3-small-8k-instruct, Qwen-7B-Chat, stablelm-2-12b-chat (previously)</span>
            <span class="k">if</span> <span class="n">stop_token_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stop_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;When constructing TokenizerInfo from a huggingface tokenizer, &quot;</span>
                        <span class="s2">&quot;stop_token_ids is neither provided by user nor found from the tokenizer. &quot;</span>
                        <span class="s2">&quot;It will be automatically detected.&quot;</span>
                    <span class="p">)</span>
            <span class="k">return</span> <span class="n">TokenizerInfo</span><span class="p">(</span>
                <span class="n">encoded_vocab</span><span class="p">,</span>
                <span class="n">VocabType</span><span class="o">.</span><span class="n">RAW</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">stop_token_ids</span><span class="o">=</span><span class="n">stop_token_ids</span><span class="p">,</span>
                <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">_is_sentencepiece_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">):</span>
            <span class="c1"># sentencepiece tokenizer</span>
            <span class="c1"># e.g. Chatglm3-6b</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;sp_model&quot;</span><span class="p">):</span>
                <span class="n">sp_model</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">sp_model</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;sp_model&quot;</span><span class="p">):</span>
                <span class="n">sp_model</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">sp_model</span>

            <span class="k">if</span> <span class="n">stop_token_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stop_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">eos_id</span> <span class="o">=</span> <span class="n">sp_model</span><span class="o">.</span><span class="n">eos_id</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">eos_id</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                        <span class="n">stop_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_id</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;When constructing TokenizerInfo from a huggingface tokenizer, &quot;</span>
                            <span class="s2">&quot;stop_token_ids is neither provided by user nor found from the tokenizer. &quot;</span>
                            <span class="s2">&quot;It will be automatically detected.&quot;</span>
                        <span class="p">)</span>
            <span class="c1"># detect vocab_type of tokenizer</span>
            <span class="k">if</span> <span class="s2">&quot;&lt;0x0A&gt;&quot;</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="p">:</span>
                <span class="n">vocab_type</span> <span class="o">=</span> <span class="n">VocabType</span><span class="o">.</span><span class="n">BYTE_FALLBACK</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">vocab_type</span> <span class="o">=</span> <span class="n">VocabType</span><span class="o">.</span><span class="n">RAW</span>

            <span class="k">return</span> <span class="n">TokenizerInfo</span><span class="p">(</span>
                <span class="n">encoded_vocab</span><span class="p">,</span>
                <span class="n">vocab_type</span><span class="o">=</span><span class="n">vocab_type</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">stop_token_ids</span><span class="o">=</span><span class="n">stop_token_ids</span><span class="p">,</span>
                <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO(yixin): unsupported tokenizer</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported tokenizer type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">vocab_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">VocabType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The type of the vocabulary.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">VocabType</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">vocab_type</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The size of the vocabulary.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">add_prefix_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the tokenizer will prepend a space before the text in the tokenization</span>
<span class="sd">        process.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">add_prefix_space</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">prepend_space_in_tokenization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the tokenizer will prepend a space before the text in the tokenization</span>
<span class="sd">        process.</span>

<span class="sd">        This property is deprecated. Use add_prefix_space instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;prepend_space_in_tokenization is deprecated. Use add_prefix_space instead.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_prefix_space</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">decoded_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bytes</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The decoded vocabulary of the tokenizer. This converts the tokens in the LLM&#39;s</span>
<span class="sd">        vocabulary back to the original format of the input text. E.g. for type ByteFallback,</span>
<span class="sd">        the token &lt;0x1B&gt; is converted back to &quot;\u001b&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">decoded_vocab</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">stop_token_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The stop token ids.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">stop_token_ids</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">special_token_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The special token ids. Special tokens include control tokens, reserved tokens,</span>
<span class="sd">        padded tokens, etc. Now it is automatically detected from the vocabulary.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">special_token_ids</span>

<div class="viewcode-block" id="TokenizerInfo.dump_metadata"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo.dump_metadata">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">dump_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Dump the metadata of the tokenizer to a json string. It can be used to construct the</span>
<span class="sd">        tokenizer info from the vocabulary and the metadata string.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">dump_metadata</span><span class="p">()</span></div>

<div class="viewcode-block" id="TokenizerInfo.from_vocab_and_metadata"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo.from_vocab_and_metadata">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_vocab_and_metadata</span><span class="p">(</span>
        <span class="n">encoded_vocab</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="n">metadata</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TokenizerInfo&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Construct the tokenizer info from the vocabulary and the metadata string in json</span>
<span class="sd">        format.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        encoded_vocab : List[Union[bytes, str]]</span>
<span class="sd">            The encoded vocabulary of the tokenizer.</span>

<span class="sd">        metadata : str</span>
<span class="sd">            The metadata string in json format.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">_create_from_handle</span><span class="p">(</span>
            <span class="n">_core</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">from_vocab_and_metadata</span><span class="p">(</span><span class="n">encoded_vocab</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span>
        <span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_detect_metadata_from_hf</span><span class="p">(</span><span class="n">backend_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Detect the metadata from the huggingface tokenizer backend string. For implementation</span>
<span class="sd">        use only.</span>

<span class="sd">        It returns {&quot;vocab_type&quot;: VocabType, &quot;add_prefix_space&quot;: bool}.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># the metadata_str should in the format of {&quot;vocab_type&quot;: int, &quot;add_prefix_space&quot;: bool}</span>
        <span class="n">metadata_str</span> <span class="o">=</span> <span class="n">_core</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">_detect_metadata_from_hf</span><span class="p">(</span><span class="n">backend_str</span><span class="p">)</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">metadata_str</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;vocab_type&quot;</span><span class="p">:</span> <span class="n">VocabType</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;vocab_type&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;add_prefix_space&quot;</span><span class="p">:</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;add_prefix_space&quot;</span><span class="p">],</span>
        <span class="p">}</span>

<div class="viewcode-block" id="TokenizerInfo.serialize_json"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo.serialize_json">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">serialize_json</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Serialize the tokenizer_info to a JSON string.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">serialize_json</span><span class="p">()</span></div>

<div class="viewcode-block" id="TokenizerInfo.deserialize_json"><a class="viewcode-back" href="../../api/python/tokenizer_info.html#xgrammar.TokenizerInfo.deserialize_json">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">deserialize_json</span><span class="p">(</span>
        <span class="n">json_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">encoded_vocab</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TokenizerInfo&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Deserialize a grammar from a JSON string.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">_create_from_handle</span><span class="p">(</span>
            <span class="n">_core</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">deserialize_json</span><span class="p">(</span><span class="n">json_string</span><span class="p">,</span> <span class="n">encoded_vocab</span><span class="p">)</span>
        <span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          

<footer>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 XGrammar</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="XGrammar Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="1041" async></script>
    
</body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   


    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="XGrammar Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="1041" async></script>
    
</body>
</html>