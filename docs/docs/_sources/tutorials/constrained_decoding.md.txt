# Constrained Decoding

Constrained decoding is a technique used by XGrammar to generate structured outputs.

In each step of LLM inference, XGrammar will provide a token mask to the LLM. The mask allows the LLM to generate tokens that follow the grammar, and prohibits those not.  The mask is a binary mask of the same length as the vocabulary size. In the sampling stage of LLM inference, the mask is used so that the sampled tokens must be valid in the mask.

![Constrained Decoding](https://raw.githubusercontent.com/mlc-ai/XGrammar-web-assets/refs/heads/main/tutorials/constrained_decoding.png)

Let's take a closer look. The binary mask applies to the logits of the LLM. It sets the logits of the tokens that are not allowed to $-\infty$, so that their probability will be $0$ after softmax. Then the sampler will sample from the vaild tokens with probability $>0$.

![Constrained Decoding Logits](https://raw.githubusercontent.com/mlc-ai/XGrammar-web-assets/refs/heads/main/tutorials/constrained_decoding_logits.png)

## XGrammar's Optimization

In XGrammar, the mask is compressed into a bitset for storage. In multi-batch settings, it also supports batching the mask. Therefore, the token mask is a tensor of shape `(batch_size, ceil(vocab_size / 32))` and dtype `int32`.
