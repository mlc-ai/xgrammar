





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Integration with LLM Engine &mdash; XGrammar 0.1.25 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/fix_text_selection.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="JSON Generation" href="json_generation.html" />
    <link rel="prev" title="Advanced Topics of the Structural Tag" href="advanced_structural_tag.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://xgrammar.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://xgrammar.mlc.ai/docs/>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/xgrammar>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://blog.mlc.ai/>Blog</a>
                </li>
             </ul>
          </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/img/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.25
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../start/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../start/quick_start.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="constrained_decoding.html">Constrained Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow_of_xgrammar.html">Workflow of XGrammar</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_topics.html">Advanced Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="structural_tag.html">Structural Tag Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_structural_tag.html">Advanced Topics of the Structural Tag</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Integration with LLM Engine</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-xgrammar">Install XGrammar</a></li>
<li class="toctree-l2"><a class="reference internal" href="#high-level-flow">High-Level Flow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#xgr-tokenizerinfo">xgr.TokenizerInfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xgr-grammarcompiler">xgr.GrammarCompiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xgr-compiledgrammar">xgr.CompiledGrammar</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xgr-grammarmatcher">xgr.GrammarMatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bitmasking-logits-in-auto-regressive-generation">Bitmasking Logits in Auto-regressive Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#structured-generation-for-batched-inference">Structured Generation for Batched Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="json_generation.html">JSON Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ebnf_guided_generation.html">EBNF-Guided Generation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">XGrammar Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../xgrammar_features/runtime_safeguards.html">Runtime Safeguards</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xgrammar_features/serialization.html">Serialization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xgrammar_features/javascript_api.html">JavaScript API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/building_docs.html">Building Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/code_coverage.html">Code Coverage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/python/index.html">XGrammar Python API</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- XGrammar -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Integration with LLM Engine</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/xgrammar/edit/main/docs/tutorials/engine_integration.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="integration-with-llm-engine">
<h1>Integration with LLM Engine<a class="headerlink" href="#integration-with-llm-engine" title="Permalink to this heading">Â¶</a></h1>
<p>XGrammar enables efficient structured generation. In this tutorial, we go over the key components
of XGrammar and how to integrate XGrammar into an LLM engine.</p>
<p>We first lay out the concepts in <a class="reference internal" href="#high-level-flow"><span class="std std-ref">High-Level Flow</span></a>.
We then demonstrate how XGrammar enables
<a class="reference internal" href="#structured-generation-for-batched-inference"><span class="std std-ref">Structured Generation for Batched Inference</span></a>.</p>
<p>The code snippets below are actual runnable code as we simulate the LLM generation.</p>
<section id="install-xgrammar">
<h2>Install XGrammar<a class="headerlink" href="#install-xgrammar" title="Permalink to this heading">Â¶</a></h2>
<p><a class="reference internal" href="../start/installation.html"><span class="doc std std-doc">XGrammar</span></a> is available via pip.
It is always recommended to install it in an isolated conda virtual environment.</p>
</section>
<section id="high-level-flow">
<h2>High-Level Flow<a class="headerlink" href="#high-level-flow" title="Permalink to this heading">Â¶</a></h2>
<p>In this section, we go over the key components of XGrammar when integrating it into an LLM engine
for structured generation.</p>
<p>First, import necessary libraries for the tutorial.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">xgrammar</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xgr</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span>
</pre></div>
</div>
<section id="xgr-tokenizerinfo">
<h3>xgr.TokenizerInfo<a class="headerlink" href="#xgr-tokenizerinfo" title="Permalink to this heading">Â¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code> is a per-model construct that encapsulates tokenizer information, including
all its vocabulary. There are several ways of instantiating it, and the most convenient way
is using an <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code>. Note that for some models, <code class="docutils literal notranslate"><span class="pre">AutoConfig.vocab_size</span></code> can be larger
than <code class="docutils literal notranslate"><span class="pre">AutoTokenizer.vocab_size</span></code> due to paddings, with the former being the shape of the modelâs
logits. To be safe, always pass in the former when instantiating <code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get tokenizer info</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="c1"># This can be larger than tokenizer.vocab_size due to paddings</span>
<span class="n">full_vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">tokenizer_info</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">full_vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xgr-grammarcompiler">
<h3>xgr.GrammarCompiler<a class="headerlink" href="#xgr-grammarcompiler" title="Permalink to this heading">Â¶</a></h3>
<p>With an <code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code>, we can instantiate an <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code>. This is a construct
that compiles a grammar according to the modelâs tokenizer info. Therefore, for each model, you
can use the same <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code> persistently, as it can compile different grammars for
the same <code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code>. Note that the <code class="docutils literal notranslate"><span class="pre">compiler</span></code> behavior can be configured with
<code class="docutils literal notranslate"><span class="pre">max_threads</span></code> for multithreading, and <code class="docutils literal notranslate"><span class="pre">enable_cache</span></code> (defaults to true) for caching
compiled grammars.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compiler</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarCompiler</span><span class="p">(</span><span class="n">tokenizer_info</span><span class="p">,</span> <span class="n">max_threads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xgr-compiledgrammar">
<h3>xgr.CompiledGrammar<a class="headerlink" href="#xgr-compiledgrammar" title="Permalink to this heading">Â¶</a></h3>
<p>Then, using the <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code>, we can compile a grammar, with the result being an
<code class="docutils literal notranslate"><span class="pre">xgr.CompiledGrammar</span></code>. Here we use a built-in JSON grammar. For other grammars, see
<a class="reference internal" href="json_generation.html"><span class="std std-doc">JSON Generation</span></a> and <a class="reference internal" href="ebnf_guided_generation.html"><span class="std std-doc">EBNF-Guided Generation</span></a>.
Every thing we have seen up to now are per-model (rather than per-generation).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compiled_grammar</span><span class="p">:</span> <span class="n">xgr</span><span class="o">.</span><span class="n">CompiledGrammar</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">compile_builtin_json_grammar</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="xgr-grammarmatcher">
<h3>xgr.GrammarMatcher<a class="headerlink" href="#xgr-grammarmatcher" title="Permalink to this heading">Â¶</a></h3>
<p>With the compiled grammar, we can instantiate a <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code>. It is the main construct
an LLM engine interacts with that maintains the state of the structured generation. Note that
each request should have its own <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code> since each has a different generation state,
as we will see in <a class="reference internal" href="#structured-generation-for-batched-inference"><span class="std std-ref">Structured Generation for Batched Inference</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate grammar matcher with the compiled grammar</span>
<span class="n">matcher</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarMatcher</span><span class="p">(</span><span class="n">compiled_grammar</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bitmasking-logits-in-auto-regressive-generation">
<h3>Bitmasking Logits in Auto-regressive Generation<a class="headerlink" href="#bitmasking-logits-in-auto-regressive-generation" title="Permalink to this heading">Â¶</a></h3>
<p>Now we simulate a single-request auto-regressive generation. See later section for
<a class="reference internal" href="#structured-generation-for-batched-inference"><span class="std std-ref">Structured Generation for Batched Inference</span></a>.</p>
<p>First, we pre-allocate a token bitmask with <code class="docutils literal notranslate"><span class="pre">xgr.allocate_token_bitmask()</span></code>,
which is essentially a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">vocab_size)</span></code>. You can also
use your own implementation for allocating a bitmask.</p>
<p>In each auto-regressive step, we fill the token bitmask according to the current state
of the matcher with <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher.fill_next_token_bitmask()</span></code>. Then, we apply the bitmask
into the modelâs logits with <code class="docutils literal notranslate"><span class="pre">xgr.apply_token_bitmask_inplace()</span></code>, which calls a CUDA kernel
if <code class="docutils literal notranslate"><span class="pre">logits</span></code> is on CUDA (recommended), otherwise a CPU implementation.</p>
<p>After masking, the logits for illegal tokens are set to negative infinity, so that
we will never sample them. After sampling the token, update the <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code>âs state with
<code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher.accept_token()</span></code>. Finally, use  <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher.reset()</span></code> to prepare
for the next generation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we simulate a valid sampled response</span>
<span class="n">sim_sampled_response</span> <span class="o">=</span> <span class="s1">&#39;{ &quot;library&quot;: &quot;xgrammar&quot; }&lt;|end_of_text|&gt;&#39;</span>
<span class="n">sim_sampled_token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sim_sampled_response</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Allocate a token bitmask</span>
<span class="n">token_bitmask</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">allocate_token_bitmask</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tokenizer_info</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>

<span class="c1"># Each loop iteration is a simulated auto-regressive step</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sim_token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sim_sampled_token_ids</span><span class="p">):</span>
    <span class="c1"># LLM inference to get logits, here we use randn to simulate.</span>
    <span class="c1"># logits is a tensor of shape (full_vocab_size,) on GPU</span>
    <span class="c1"># logits = LLM.inference()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">full_vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># Apply bitmask to logits to mask invalid tokens</span>
    <span class="n">matcher</span><span class="o">.</span><span class="n">fill_next_token_bitmask</span><span class="p">(</span><span class="n">token_bitmask</span><span class="p">)</span>
    <span class="n">xgr</span><span class="o">.</span><span class="n">apply_token_bitmask_inplace</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">token_bitmask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># Sample next token</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">full_vocab_size</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="c1"># Accept token from matcher to update its state, so that the next bitmask</span>
    <span class="c1"># generated will enforce the next token to be generated. Assert to make</span>
    <span class="c1"># sure the token is indeed valid. Here we accept the simulated response</span>
    <span class="c1"># assert matcher.accept_token(next_token_id)</span>
    <span class="k">assert</span> <span class="n">matcher</span><span class="o">.</span><span class="n">accept_token</span><span class="p">(</span><span class="n">sim_token_id</span><span class="p">)</span>

<span class="c1"># Since we accepted a stop token `&lt;|end_of_text|&gt;`, we have terminated</span>
<span class="k">assert</span> <span class="n">matcher</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">()</span>

<span class="c1"># Reset to be ready for the next auto-regressive generation</span>
<span class="n">matcher</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="structured-generation-for-batched-inference">
<h2>Structured Generation for Batched Inference<a class="headerlink" href="#structured-generation-for-batched-inference" title="Permalink to this heading">Â¶</a></h2>
<p>The code snippets above assume a single request generation.
This section demonstrates how the same concept works with batched generation.</p>
<p>First, follow the exact same steps above for the per-model constructs
<code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code> and <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code>. Say each request needs
to generate a valid JSON.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">xgrammar</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xgr</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span>

<span class="c1"># Get tokenizer info</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="c1"># This can be larger than tokenizer.vocab_size due to paddings</span>
<span class="n">full_vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">tokenizer_info</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">full_vocab_size</span><span class="p">)</span>

<span class="c1"># Compile a JSON grammar</span>
<span class="n">compiler</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarCompiler</span><span class="p">(</span><span class="n">tokenizer_info</span><span class="p">,</span> <span class="n">max_threads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">compiled_grammar</span><span class="p">:</span> <span class="n">xgr</span><span class="o">.</span><span class="n">CompiledGrammar</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">compile_builtin_json_grammar</span><span class="p">()</span>
</pre></div>
</div>
<p>Now, we need to maintain an <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code> for each request in the batch, since
each has a different generation state. Note that each request in the batch can follow a different
<code class="docutils literal notranslate"><span class="pre">xgr.CompiledGrammar</span></code>, but here for simplicity, they are all just following the general
JSON grammar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">matchers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarMatcher</span><span class="p">(</span><span class="n">compiled_grammar</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">token_bitmask</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">allocate_token_bitmask</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tokenizer_info</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
<p>We simulate an auto-regressive generation of batched inference. Note that here we
assume the generation lengths of the two requests are the same for simplicity. But
it should be easy to generalize based on how your engine supports batched inference.
The key difference from single-request generation is that, in batched-request generation,
each request has its own <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code> to maintain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sim_sampled_responses</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;{&quot;name&quot;: &quot;a&quot;}&lt;|end_of_text|&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;{&quot;name&quot;: &quot;b&quot;}&lt;|end_of_text|&gt;&#39;</span><span class="p">]</span>
<span class="n">sim_sampled_token_ids</span> <span class="o">=</span> <span class="p">[</span>
  <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">sim_sampled_responses</span>
<span class="p">]</span>

<span class="c1"># Each loop iteration is a simulated auto-regressive step</span>
<span class="k">for</span> <span class="n">loop_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sim_sampled_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="c1"># LLM batched inference to get logits, here we use randn to simulate</span>
    <span class="c1"># Now, logits is a tensor of shape (batch_size, full_vocab_size) on GPU</span>
    <span class="c1"># logits = LLM.inference()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">full_vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># This for loop is parallelizable using threading.Thread. But estimate</span>
    <span class="c1"># the overhead in your engine.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_next_token_bitmask</span><span class="p">(</span><span class="n">token_bitmask</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">xgr</span><span class="o">.</span><span class="n">apply_token_bitmask_inplace</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">token_bitmask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># Sample next token</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">next_token_ids</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">full_vocab_size</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Update the matcher for each request</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># Here we accept the simulated response</span>
        <span class="c1"># assert matchers[i].accept_token(next_token_ids[i])</span>
        <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">accept_token</span><span class="p">(</span><span class="n">sim_sampled_token_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">loop_iter</span><span class="p">])</span>

<span class="c1"># In our simulated case, all requests should have terminated since we accepted</span>
<span class="c1"># a stop token `&lt;|end_of_text|&gt;`</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">()</span>
    <span class="c1"># Reset to be ready for the next generation</span>
    <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="json_generation.html" class="btn btn-neutral float-right" title="JSON Generation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="advanced_structural_tag.html" class="btn btn-neutral float-left" title="Advanced Topics of the Structural Tag" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">Â© 2024 XGrammar</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  
    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="XGrammar Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="1041" async></script>
    
</body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   


    <!-- RunLLM Widget Script -->
    <script type="module" id="runllm-widget-script" src="https://widget.runllm.com" crossorigin="true" version="stable" runllm-keyboard-shortcut="Mod+j" runllm-name="XGrammar Chatbot" runllm-position="BOTTOM_RIGHT" runllm-assistant-id="1041" async></script>
    
</body>
</html>